{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import tiktoken \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objects as go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "# elif torch.backends.mps.is_available():\n",
    "#   device = 'mps'\n",
    "else :\n",
    "  device = 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 :  Fetch The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 characters: I HAD always thought\n",
      "Number of Characters: 20479\n"
     ]
    }
   ],
   "source": [
    "with open('the-verdict.txt','r',encoding='utf-8') as f:\n",
    "  raw_text = f.read()\n",
    "\n",
    "print(f'First 20 characters: {raw_text[:20]}')\n",
    "print(f'Number of Characters: {len(raw_text)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Dataset Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  def __init__(self,raw_text,tokenizer,context_window,stride):\n",
    "    self.token_id = tokenizer.encode(raw_text)\n",
    "    self.X = []\n",
    "    self.y = []\n",
    "\n",
    "    for i in range(0,len(self.token_id)-context_window,stride):\n",
    "      input_chunks = self.token_id[i:i+context_window]\n",
    "      output_chunks = self.token_id[(i)+1:(i+context_window) +1]\n",
    "      self.X.append(torch.tensor(input_chunks))\n",
    "      self.y.append(torch.tensor(output_chunks))\n",
    "\n",
    "  def __len__(self):return len(self.X)\n",
    "\n",
    "  def __getitem__(self,idx):return self.X[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split \n",
    "\n",
    "Notice we split the corpus to train/val before creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.9\n",
    "split_index = int(len(raw_text)*0.9)\n",
    "\n",
    "train_text = raw_text[:split_index]\n",
    "val_text = raw_text[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 4612\n",
      "Number of tokens 534\n"
     ]
    }
   ],
   "source": [
    "train_ds = Data(train_text,tiktoken.get_encoding('gpt2'),context_window=256,stride=128)\n",
    "val_ds   = Data(val_text,tiktoken.get_encoding('gpt2'),context_window=256,stride=128)\n",
    "\n",
    "print(f'Number of tokens {len(train_ds.token_id)}')\n",
    "print(f'Number of tokens {len(val_ds.token_id)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,batch_size=2,shuffle=False,drop_last=True,num_workers=0)\n",
    "val_dl   = DataLoader(val_ds,batch_size=2,shuffle=False,drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl))\n",
    "print(len(val_dl))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "2\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "3\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "4\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "5\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "6\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "7\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "8\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "9\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "10\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "11\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "12\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "13\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "14\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "15\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "16\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n",
      "17\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(train_dl):\n",
    "  print(i+1)\n",
    "  print(f'X : {x.shape}')\n",
    "  print(f'y : {y.shape}')\n",
    "  print('---'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "X : torch.Size([2, 256])\n",
      "y : torch.Size([2, 256])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(val_dl):\n",
    "  print(i+1)\n",
    "  print(f'X : {x.shape}')\n",
    "  print(f'y : {y.shape}')\n",
    "  print('---'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions \n",
    "\n",
    "These conversion functions allow us to convert text to token_ids,and vice versa easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text,tokenizer=tiktoken.get_encoding('gpt2')):\n",
    "  token_ids = tokenizer.encode(text,allowed_special={'<|endoftext|>'})              # python list.\n",
    "  token_ids_in_tensors = torch.tensor(token_ids).unsqueeze(0)                       # tensor,with batch dimesion.\n",
    "  return token_ids_in_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,   428,   318,   387,  5767,   220, 50256]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_token_ids('Hello this is hawar <|endoftext|>')    # Note : the last token_id is 50256, (50257) is the vocab size by the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids_in_tensors,tokenizer=tiktoken.get_encoding('gpt2')):\n",
    "  token_ids = token_ids_in_tensors.squeeze(0).tolist()    # Note: It can only handle batch_size = 1\n",
    "  text = tokenizer.decode(token_ids)\n",
    "  return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,d_in,d_out,\n",
    "               context_window,    # The context window is needed here to create the mask.\n",
    "               num_heads,\n",
    "               dp_attn,qkv_bias=False):\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    assert d_out%num_heads == 0, 'd_out must be divisble by num_heads'\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = self.d_out //self.num_heads # dimension of each head\n",
    "\n",
    "    self.W_q = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "    self.W_k = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "    self.W_v = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "\n",
    "    self.register_buffer('mask',torch.triu(torch.ones(context_window,context_window),diagonal=1))\n",
    "    # why shape (context_window,context_window) ? beacause we'll use it on Q@KT which will have shape of (b,self.num_heads,context_window,context_window)\n",
    "    # But since we might have different num_tokens(i.e num_tokens is not context_window for every case) we will slice it to [:num_tokens][:num_tokens]\n",
    "    # This slicing wont be necessary if num_tokens == context window\n",
    "    \n",
    "    self.dropout = nn.Dropout(dp_attn)\n",
    "    self.out_proj = nn.Linear(d_out,d_out)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    b,num_tokens,d_in = x.shape\n",
    "\n",
    "    Q = self.W_q(x)\n",
    "    K = self.W_k(x)\n",
    "    V = self.W_v(x)\n",
    "\n",
    "    # split \n",
    "    Q = Q.view(b,num_tokens,self.num_heads,self.head_dim).transpose(1,2)  # result : (b,self.num_heads,num_tokens,self.head_dim)\n",
    "    K = K.view(b,num_tokens,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    V = V.view(b,num_tokens,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "    # \n",
    "    attn_score = Q @ K.transpose(2,3) # result : (b,self.num_head,num_tokens,num_tokens)\n",
    "    attn_score.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)\n",
    "    attn_weight = torch.softmax(attn_score/(K.shape[-1]**0.5),dim=-1)\n",
    "    attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "    context_vec = attn_weight @ V # result : (b,self.num_head,num_toknes,self.head_dim)\n",
    "    context_vec = context_vec.transpose(1,2).contiguous().view(b,num_tokens,self.d_out)\n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(cfg['d_out'])\n",
    "    self.attn = MultiHeadAttention(cfg['d_in'],cfg['d_out'],cfg['context_window'],cfg['num_heads'],cfg['dp_attn'],cfg['qkv_bias'])\n",
    "    self.dp_after_attn = nn.Dropout(cfg['dp_after_trans'])\n",
    "\n",
    "    self.norm2 = nn.LayerNorm(cfg['d_out'])\n",
    "    self.fc = nn.Sequential(nn.Linear(cfg['d_out'],4*cfg['d_out']),\n",
    "                            nn.GELU(),\n",
    "                            nn.Linear(4*cfg['d_out'],cfg['d_out']))\n",
    "    self.dp_after_fc = nn.Dropout(cfg['dp_after_fc'])\n",
    "\n",
    "  def forward(self,x):\n",
    "    shortcut = x  \n",
    "    x = self.norm1(x)\n",
    "    x = self.attn(x)\n",
    "    x = self.dp_after_attn(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.fc(x)\n",
    "    x = self.dp_after_fc(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    return x \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = {    #TransformerBlock Level\n",
    "#           'd_in':12,\n",
    "#            'd_out':12,\n",
    "#            'context_window':256,  # You can not have num_tokens greater than this number \n",
    "#            'num_heads':4,\n",
    "\n",
    "#            'dp_attn':0.0,\n",
    "#            'dp_after_trans':0.0,\n",
    "#            'dp_after_fc':0.0,\n",
    "\n",
    "#            'qkv_bias':False,\n",
    "           \n",
    "#             # GPTModel Level\n",
    "#            'dp_before_trans':0.0,\n",
    "#            'vocab_size':50256+1,\n",
    "#            'num_layers':4}\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "GPT_CONFIG_124M = {    #TransformerBlock Level\n",
    "          'd_in':768,\n",
    "           'd_out':768,\n",
    "           'context_window':256,  # You can not have num_tokens greater than this number \n",
    "           'num_heads':12,\n",
    "           \n",
    "           'dp_attn':0.1,\n",
    "           'dp_after_attn':0.1,\n",
    "           'dp_after_fc':0.1,\n",
    "\n",
    "           'qkv_bias':False,\n",
    "           \n",
    "            # GPTModel Level\n",
    "           'dp_before_trans':0.1,\n",
    "           'vocab_size':50256+1,\n",
    "           'num_layers':12}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 : GPT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['d_in'])\n",
    "    self.pos_emb = nn.Embedding(cfg['context_window'],cfg['d_in'])\n",
    "    self.dp_before_trans = nn.Dropout(cfg['dp_before_trans'])\n",
    "    self.transformer_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['num_layers'])])\n",
    "    self.final_norm = nn.LayerNorm(cfg['d_out'])\n",
    "    self.head_out = nn.Linear(cfg['d_out'],cfg['vocab_size'])\n",
    "\n",
    "  def forward(self,x):\n",
    "    b,num_tokens = x.shape\n",
    "\n",
    "    tok_em = self.tok_emb(x)\n",
    "    pos_em = self.pos_emb(torch.arange(num_tokens,device=device))   # this is where it can handle dynamic sequence lengths (as long as it's lower than context window)\n",
    "    x = tok_em + pos_em\n",
    "\n",
    "    x = self.dp_before_trans(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.head_out(x)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0006e-02, -1.4220e-01, -2.0487e-01,  ..., -5.5046e-01,\n",
      "          -3.0857e-01,  6.0228e-01],\n",
      "         [ 2.7873e-01,  2.6370e-02,  9.4051e-01,  ...,  5.1697e-01,\n",
      "          -1.5594e-01,  3.2128e-01],\n",
      "         [ 8.0839e-01,  3.8975e-01, -1.9603e-01,  ...,  7.4751e-01,\n",
      "          -3.7116e-01,  1.7367e-01],\n",
      "         ...,\n",
      "         [ 1.8855e-01, -2.3749e-01,  6.4262e-02,  ...,  3.1803e-01,\n",
      "           6.3902e-02,  6.4671e-01],\n",
      "         [ 7.1463e-01,  6.5541e-01,  6.7452e-01,  ..., -4.2267e-01,\n",
      "          -1.7147e-01,  4.5561e-01],\n",
      "         [-2.5878e-01,  6.5790e-01,  6.3138e-01,  ...,  3.2134e-01,\n",
      "           4.2317e-01, -9.0939e-01]],\n",
      "\n",
      "        [[-3.3346e-01,  1.3003e-01, -2.4412e-01,  ..., -1.4811e-01,\n",
      "           8.4797e-01,  3.1190e-01],\n",
      "         [ 6.3580e-01,  2.9483e-01,  8.3385e-01,  ...,  7.6870e-01,\n",
      "          -2.6883e-01, -9.0828e-01],\n",
      "         [ 6.8660e-01, -5.6592e-03, -7.7130e-01,  ..., -2.7326e-01,\n",
      "          -8.0615e-01, -1.0252e+00],\n",
      "         ...,\n",
      "         [ 3.6490e-01, -7.0258e-02, -1.0969e+00,  ..., -7.3049e-01,\n",
      "           1.2755e+00,  3.8426e-01],\n",
      "         [ 7.6898e-01,  2.1525e+00,  9.1751e-01,  ..., -5.8337e-02,\n",
      "           3.4236e-01,  8.8291e-01],\n",
      "         [-9.1322e-05,  8.8759e-01,  1.7456e-04,  ...,  9.7277e-02,\n",
      "           6.2242e-01, -9.9340e-01]]])\n",
      "torch.Size([2, 256, 50257])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  print(model(torch.randint(0,50257,(2,256)).to(device)))        # torch.randint(0,vocab_size,(batch_size,num_tokens))     PS: num_tokens must be <= context_window\n",
    "  print(model(torch.randint(0,50257,(2,256)).to(device)).shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 : Next token  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we dont slice the mask, it will throw an error here, why?  beacuase the input num_tokens != context_window \n",
    "\n",
    "def generate_simple_text(starting_text,\n",
    "                         context_window,    # this can be anything between [1,context_window]\n",
    "                         num_tokens_generated = 20,temperature=1,k=None):  \n",
    "  \n",
    "    starting_tokens = text_to_token_ids(starting_text).to(device)       # this will have the shape (B=1,number of tokens after converting text to token ids)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_tokens_generated):\n",
    "            starting_tokens = starting_tokens[:,-context_window:]    # how far back to pay attention \n",
    "            logits = model(starting_tokens)[:,-1,:]                  # we only want the last vector\n",
    "\n",
    "            # new\n",
    "            if k is not None:\n",
    "              top_logits, _ = torch.topk(logits,k)                                      # find top 3. \n",
    "              new_logits = torch.where(logits<top_logits[:,-1],-torch.inf,logits)       # keep top 3, make other -inf.\n",
    "              probs = torch.softmax(new_logits/temperature,dim=-1)                                  # probability of top 3 , the rest will be zero.\n",
    "              token_predicted = torch.multinomial(probs,num_samples = 1)                # pick one (according to their probability) from top 3.\n",
    "            # End\n",
    "            else : \n",
    "                token_predicted = torch.argmax(logits,dim=-1,keepdim=True)\n",
    "            \n",
    "            starting_tokens = torch.cat([starting_tokens,token_predicted],dim=-1)\n",
    "            \n",
    "        print(token_ids_to_text(starting_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everty effort moves you isolate agency Thunderboltapego attributes LynchTemplate cleanogi Haitiumn html wrongly Regist cherry Porterressing shows scourge Thoughts\n"
     ]
    }
   ],
   "source": [
    "generate_simple_text(starting_text='Everty effort moves you',context_window=GPT_CONFIG_124M['context_window'],temperature=0.2,k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 : Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay =0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 8.353      Validation Loss : 6.83\n",
      "Everty effort moves you to he was I I I he, and, he was, I I.\n",
      "\" to I\n",
      "======================================================================================================================================================\n",
      "Training Loss : 5.73       Validation Loss : 6.213\n",
      "Everty effort moves you to the to the the picture--as he was a and he had I had the picture--and\n",
      "======================================================================================================================================================\n",
      "Training Loss : 4.509      Validation Loss : 5.941\n",
      "Everty effort moves you of it.\n",
      "\n",
      "\"Oh--his, and I had been--and I was a to\n",
      "======================================================================================================================================================\n",
      "Training Loss : 3.262      Validation Loss : 5.93\n",
      "Everty effort moves you say, and he said--and by me, the picture.\n",
      "He was I was not to\n",
      "======================================================================================================================================================\n",
      "Training Loss : 2.29       Validation Loss : 6.055\n",
      "Everty effort moves you say it. I don't think of a pale yellow or _rose Dubarry_ drawing-rooms\n",
      "======================================================================================================================================================\n",
      "Training Loss : 1.571      Validation Loss : 6.3\n",
      "Everty effort moves you just a little under a picture for nothing--though a little: \"sweet.\n",
      "\"Oh,\n",
      "======================================================================================================================================================\n",
      "Training Loss : 1.04       Validation Loss : 6.45\n",
      "Everty effort moves you'd never touched a brush.\"\n",
      "\n",
      "\"I didn't bear not till I had always.\n",
      "\n",
      "======================================================================================================================================================\n",
      "Training Loss : 0.753      Validation Loss : 6.7\n",
      "Everty effort moves you say by a little Claude a little quickly.\n",
      "\n",
      "\"What a wonder!\n",
      "\n",
      "\"Why\n",
      "======================================================================================================================================================\n",
      "Training Loss : 0.606      Validation Loss : 6.917\n",
      "Everty effort moves you'd never touched a little wild--I felt nervous and uncertain.\n",
      "\n",
      "\"Once, had lit\n",
      "======================================================================================================================================================\n",
      "Training Loss : 0.527      Validation Loss : 7.019\n",
      "Everty effort moves you say to go a brush.\"\n",
      "\n",
      "\n",
      "\"Never think of ever having been used as a dozen\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "epochs = 10\n",
    "TRAIN_LOSS,VAL_LOSS = [],[]\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "  train_loss,val_loss = [],[]\n",
    "\n",
    "  # TRAINING \n",
    "  model.train()\n",
    "  for x,y in train_dl:\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x)\n",
    "    loss = loss_fn(y_hat.flatten(0,1),y.flatten())    # y_hat result : (512,50257),    y result : (512) \n",
    "    train_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  mean_epoch_train_loss = round(torch.mean(torch.tensor(train_loss)).item(),3)\n",
    "  print(f'Training Loss : {mean_epoch_train_loss:<10}',end=' ')\n",
    "  TRAIN_LOSS.append(mean_epoch_train_loss)\n",
    "\n",
    "  # VALIDATION \n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for x,y in val_dl:\n",
    "      x,y = x.to(device),y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = loss_fn(y_hat.flatten(0,1),y.flatten())\n",
    "      val_loss.append(loss.item())\n",
    "\n",
    "    mean_epoch_val_loss = round(torch.tensor(val_loss).mean().item(),3)\n",
    "    print(f'Validation Loss : {mean_epoch_val_loss}')\n",
    "    VAL_LOSS.append(mean_epoch_val_loss)\n",
    "\n",
    "  # Generate text \n",
    "  generate_simple_text(starting_text='Everty effort moves you',context_window=GPT_CONFIG_124M['context_window'],num_tokens_generated = 20,k=3)\n",
    "\n",
    "  print('======'*25)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8 : Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          8.353,
          5.73,
          4.509,
          3.262,
          2.29,
          1.571,
          1.04,
          0.753,
          0.606,
          0.527
         ]
        },
        {
         "line": {
          "color": "yellow"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "y": [
          6.83,
          6.213,
          5.941,
          5.93,
          6.055,
          6.3,
          6.45,
          6.7,
          6.917,
          7.019
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_curve = go.Scatter(\n",
    "  x = torch.arange(epochs),\n",
    "  y = TRAIN_LOSS,\n",
    "  mode = 'lines',\n",
    "  line = dict(color='red'),\n",
    "  name = 'Training Loss'\n",
    ")\n",
    "\n",
    "valid_curve = go.Scatter(\n",
    "  x = torch.arange(epochs),\n",
    "  y = VAL_LOSS,\n",
    "  mode = 'lines',\n",
    "  line = dict(color='yellow'),\n",
    "  name = 'Validation Loss'\n",
    ")\n",
    "\n",
    "figure = go.Figure(data=[training_curve,valid_curve])\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Saving model weights in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "  {'model_state_dict':model.state_dict(),\n",
    "   'optimizer_state_dict':optimizer.state_dict()},\n",
    "   'model_and_optimizers.pth'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/xcj55x9n0hs0jx2pcwy46_mc0000gn/T/ipykernel_51627/680644273.py:1: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('model_and_optimizers.pth',map_location=device)\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay =0.1)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
