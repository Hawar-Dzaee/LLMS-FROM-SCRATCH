{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic \n",
    "\n",
    "This notebook reproduces pytorch's built-in module `nn.MultiheadAttention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to implement MultiheadAttention from scratch is crucial for grasping its core mechanics. However, once mastered, it's more practical to use pre-built implementations for better code maintainability, readability, and optimized performance.However, it's essential to verify how these pre-built components work under the hood to effectively adapt them for different use cases and avoid unexpected behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration\n",
    "  - Identical hyperparameters and inputs used for both parts\n",
    "\n",
    "\n",
    "Implementation Methods\n",
    "\n",
    "  - Part One: PyTorch's Built-in Implementation\n",
    "  - Part Two: Custom Implementation from Scratch\n",
    "\n",
    "\n",
    "Validation\n",
    "\n",
    "  - Results comparison between Part One and Part Two\n",
    "  - Verification of output equality\n",
    "\n",
    "Afterwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical Hyperparameters for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "num_tokens = 8\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identical inputs (with `torch.manual_seed`) for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# why different inputs to produce Q,K,V ?? \n",
    "# stay tunned. \n",
    "\n",
    "input_to_produce_Q = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "input_to_produce_K = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "input_to_produce_V = torch.rand(batch_size,num_tokens,embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE : Pytorch's  Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0888, -0.0024,  0.5353,  0.1906],\n",
      "        [-0.2281, -0.3698, -0.1026, -0.2641],\n",
      "        [-0.1962,  0.0293,  0.3651,  0.3328],\n",
      "        [-0.5986,  0.3796,  0.1711,  0.5809],\n",
      "        [ 0.4042, -0.5580, -0.5822, -0.2954],\n",
      "        [ 0.5377, -0.1020,  0.2621, -0.2846],\n",
      "        [ 0.6009, -0.2591,  0.4592,  0.0073],\n",
      "        [-0.3226,  0.3148, -0.3251,  0.1801],\n",
      "        [-0.1768, -0.0671, -0.5887, -0.2920],\n",
      "        [ 0.3323, -0.1489,  0.6100,  0.4909],\n",
      "        [-0.0287, -0.4087,  0.3729,  0.1901],\n",
      "        [-0.3958,  0.3978,  0.3718,  0.5431]], requires_grad=True)\n",
      "torch.Size([12, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "THe following is out projection weight\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450]], requires_grad=True)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                       num_heads=num_heads,\n",
    "                                       dropout=0,\n",
    "                                       bias = False,\n",
    "                                       add_bias_kv=False,\n",
    "                                       batch_first=True,    # important\n",
    "                                       device=None)\n",
    "\n",
    "\n",
    "\n",
    "# To grab initialized `nn.MultiheadAttention`s weights.....\n",
    "print('The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\\n')\n",
    "print(multihead_attn.in_proj_weight)\n",
    "print(multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('THe following is out projection weight\\n')\n",
    "print(multihead_attn.out_proj.weight)  # Output projection weights\n",
    "print(multihead_attn.out_proj.weight.shape)  # Output projection weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few words on `nn.MultiheadAttention` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if `bias` = True and `add_kv_bias` = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q = x \\cdot W^Q + b^Q$\n",
    "\n",
    "$K = x \\cdot W^K + b^K + \\text{bias}_{K_shared}$\n",
    "\n",
    "$V = x \\cdot W^V + b^V + \\text{bias}_{V_shared}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simplicity we'll set both `bias` and `add_kv_bias` to `False` in `nn.MultiheadAttention`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : pytorch `nn.MultiheadAttention()` does not mask (Q.K^T) by default. we have to pass the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = torch.triu(torch.ones(num_tokens,num_tokens),diagonal=1).bool()\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch's nn.MultiheadAttention is designed to be flexible for different use cases, so it requires Q, K, and V as inputs rather than assuming they will be derived from the same source every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART ONE RESULT :\n",
      "tensor([[[-0.1419,  0.5573, -0.0425, -0.2406],\n",
      "         [-0.1758,  0.5663, -0.0353, -0.2560],\n",
      "         [-0.2062,  0.6112, -0.0474, -0.2779],\n",
      "         [-0.1789,  0.5776, -0.0528, -0.2556],\n",
      "         [-0.1743,  0.5528, -0.0469, -0.2469],\n",
      "         [-0.1835,  0.5435, -0.0493, -0.2459],\n",
      "         [-0.1755,  0.5388, -0.0514, -0.2401],\n",
      "         [-0.1663,  0.5134, -0.0536, -0.2267]]])\n",
      "torch.Size([1, 8, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART ONE ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3607, 0.3203, 0.3190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2671, 0.2432, 0.2426, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2208, 0.1900, 0.1895, 0.1925, 0.2072, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1757, 0.1642, 0.1639, 0.1645, 0.1703, 0.1613, 0.0000, 0.0000],\n",
      "         [0.1585, 0.1371, 0.1363, 0.1437, 0.1519, 0.1343, 0.1381, 0.0000],\n",
      "         [0.1358, 0.1172, 0.1160, 0.1333, 0.1362, 0.1175, 0.1209, 0.1231]]])\n",
      "torch.Size([1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    PART_ONE_RESULT, attn_output_weights_part_one  = multihead_attn(input_to_produce_Q,input_to_produce_K,input_to_produce_V,\n",
    "                                                                    attn_mask = attn_mask )\n",
    "\n",
    "print('PART ONE RESULT :')\n",
    "print(PART_ONE_RESULT)\n",
    "print(PART_ONE_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART ONE ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_one)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_one.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the weights which `nn.MultiheadAttention` initialized, in our `FromScratchMultiheadAttention`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO : FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromScratchMultiheadAttention(nn.Module):\n",
    "  def __init__(self,context_window,embed_dim,num_heads,dropout=0,add_bias_kv=False,device=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # we will assume d_in == d_out and they are both embed_dim.\n",
    "\n",
    "    # Handling dimensions\n",
    "    assert embed_dim % num_heads == 0, 'Embedding must be divisible by Number of heads'\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = self.embed_dim//self.num_heads\n",
    "\n",
    "    # W_q, W_k, W_v\n",
    "    self.W_q      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_k      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_v      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.out_proj = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "\n",
    "\n",
    "    W_q,W_k,W_v = multihead_attn.in_proj_weight.chunk(3)   # pytorch's internal initialization of nn.MultiheadAttention. pytorch initialize all three (q,k,v) in a single matrix.\n",
    "    out_proj = multihead_attn.out_proj.weight\n",
    "\n",
    "    # we gonna put the initialized weight by nn.MultiheadAttention to our layers. so we can see if they will produce the same result\n",
    "    self.W_q.weight.data = W_q\n",
    "    self.W_k.weight.data = W_k\n",
    "    self.W_v.weight.data = W_v\n",
    "    self.out_proj.weight.data = out_proj\n",
    "\n",
    "\n",
    "\n",
    "    # Miscellaneous\n",
    "    self.register_buffer('mask',torch.triu(torch.ones(context_window,context_window),diagonal=1))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "  \n",
    "  def forward(self,input_to_produce_Q,input_to_produce_K,input_to_produce_V):\n",
    "    B_q,num_token_q,embed_dim_q = input_to_produce_Q.shape\n",
    "    B_k,num_token_k,embed_dim_k = input_to_produce_K.shape\n",
    "    B_v,num_token_v,embed_dim_v = input_to_produce_V.shape\n",
    "\n",
    "    Q = self.W_q(input_to_produce_Q) \n",
    "    K = self.W_k(input_to_produce_K) \n",
    "    V = self.W_v(input_to_produce_V) \n",
    "\n",
    "    # splitting \n",
    "    Q = Q.view(B_q,num_token_q,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    K = K.view(B_k,num_token_k,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    V = V.view(B_v,num_token_v,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "    # QK,mask,softmax,dropout\n",
    "    attn_score = Q @ K.transpose(2,3)\n",
    "    attn_score.masked_fill_(self.mask.bool()[:num_token_q,:num_token_k],-torch.inf)\n",
    "    attn_weight = torch.softmax(attn_score/K.shape[-1]**0.5,dim=-1)\n",
    "    attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "    # context_vec\n",
    "    context_vec = attn_weight @ V\n",
    "\n",
    "    # Putting the heads back together \n",
    "    context_vec = context_vec.transpose(1,2).contiguous().view(B_q,num_token_q,self.embed_dim)    # it doesn't matter which (B) you choose\n",
    "\n",
    "    # projection \n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec,attn_weight\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO RESULT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can think of context_window as a max num_tokens your model can process at one go. since  we are feeding the model 8 tokens (numz-tokens = 8), context_window anything above 8 will do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART TWO RESULT :\n",
      "tensor([[[-0.1419,  0.5573, -0.0425, -0.2406],\n",
      "         [-0.1758,  0.5663, -0.0353, -0.2560],\n",
      "         [-0.2062,  0.6112, -0.0474, -0.2779],\n",
      "         [-0.1789,  0.5776, -0.0528, -0.2556],\n",
      "         [-0.1743,  0.5528, -0.0469, -0.2469],\n",
      "         [-0.1835,  0.5435, -0.0493, -0.2459],\n",
      "         [-0.1755,  0.5388, -0.0514, -0.2401],\n",
      "         [-0.1663,  0.5134, -0.0536, -0.2267]]])\n",
      "torch.Size([1, 8, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART TWO ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5547, 0.4453, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3986, 0.3052, 0.2961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2869, 0.2303, 0.2247, 0.2581, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2441, 0.1714, 0.1646, 0.1933, 0.2266, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1863, 0.1589, 0.1560, 0.1677, 0.1801, 0.1511, 0.0000, 0.0000],\n",
      "          [0.1786, 0.1296, 0.1250, 0.1467, 0.1684, 0.1180, 0.1337, 0.0000],\n",
      "          [0.1533, 0.1136, 0.1099, 0.1349, 0.1499, 0.1066, 0.1203, 0.1115]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3227, 0.3354, 0.3419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2474, 0.2560, 0.2604, 0.2362, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1975, 0.2087, 0.2145, 0.1916, 0.1877, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1652, 0.1696, 0.1718, 0.1614, 0.1606, 0.1715, 0.0000, 0.0000],\n",
      "          [0.1385, 0.1445, 0.1477, 0.1407, 0.1354, 0.1507, 0.1425, 0.0000],\n",
      "          [0.1183, 0.1207, 0.1221, 0.1317, 0.1224, 0.1284, 0.1216, 0.1348]]]])\n",
      "torch.Size([1, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "mha = FromScratchMultiheadAttention(context_window=1024,   # see the above note.\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=0.)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    PART_TWO_RESULT,attn_output_weights_part_two = mha(input_to_produce_Q,input_to_produce_K,input_to_produce_V)\n",
    "\n",
    "print('PART TWO RESULT :')\n",
    "print(PART_TWO_RESULT)\n",
    "print(PART_TWO_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART TWO ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_two)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_two.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5267, 0.4733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3607, 0.3203, 0.3190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2671, 0.2432, 0.2426, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2208, 0.1900, 0.1895, 0.1925, 0.2072, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1757, 0.1642, 0.1639, 0.1645, 0.1703, 0.1613, 0.0000, 0.0000],\n",
       "         [0.1585, 0.1371, 0.1363, 0.1437, 0.1519, 0.1343, 0.1381, 0.0000],\n",
       "         [0.1358, 0.1172, 0.1160, 0.1333, 0.1362, 0.1175, 0.1209, 0.1231]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights_part_two = attn_output_weights_part_two.mean(dim=1)   # taking average across heads dimension.\n",
    "attn_output_weights_part_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `PART_ONE_RESULT` with `PART_TWO_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[-0.1419,  0.5573, -0.0425, -0.2406],\n",
      "         [-0.1758,  0.5663, -0.0353, -0.2560],\n",
      "         [-0.2062,  0.6112, -0.0474, -0.2779],\n",
      "         [-0.1789,  0.5776, -0.0528, -0.2556],\n",
      "         [-0.1743,  0.5528, -0.0469, -0.2469],\n",
      "         [-0.1835,  0.5435, -0.0493, -0.2459],\n",
      "         [-0.1755,  0.5388, -0.0514, -0.2401],\n",
      "         [-0.1663,  0.5134, -0.0536, -0.2267]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[-0.1419,  0.5573, -0.0425, -0.2406],\n",
      "         [-0.1758,  0.5663, -0.0353, -0.2560],\n",
      "         [-0.2062,  0.6112, -0.0474, -0.2779],\n",
      "         [-0.1789,  0.5776, -0.0528, -0.2556],\n",
      "         [-0.1743,  0.5528, -0.0469, -0.2469],\n",
      "         [-0.1835,  0.5435, -0.0493, -0.2459],\n",
      "         [-0.1755,  0.5388, -0.0514, -0.2401],\n",
      "         [-0.1663,  0.5134, -0.0536, -0.2267]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{PART_ONE_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{PART_TWO_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(PART_ONE_RESULT, PART_TWO_RESULT)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `attn_output_weights_part_one` with `attn_output_weights_part_two`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3607, 0.3203, 0.3190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2671, 0.2432, 0.2426, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2208, 0.1900, 0.1895, 0.1925, 0.2072, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1757, 0.1642, 0.1639, 0.1645, 0.1703, 0.1613, 0.0000, 0.0000],\n",
      "         [0.1585, 0.1371, 0.1363, 0.1437, 0.1519, 0.1343, 0.1381, 0.0000],\n",
      "         [0.1358, 0.1172, 0.1160, 0.1333, 0.1362, 0.1175, 0.1209, 0.1231]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5267, 0.4733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3607, 0.3203, 0.3190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2671, 0.2432, 0.2426, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2208, 0.1900, 0.1895, 0.1925, 0.2072, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1757, 0.1642, 0.1639, 0.1645, 0.1703, 0.1613, 0.0000, 0.0000],\n",
      "         [0.1585, 0.1371, 0.1363, 0.1437, 0.1519, 0.1343, 0.1381, 0.0000],\n",
      "         [0.1358, 0.1172, 0.1160, 0.1333, 0.1362, 0.1175, 0.1209, 0.1231]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{attn_output_weights_part_one}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{attn_output_weights_part_two}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(attn_output_weights_part_one, attn_output_weights_part_two)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afterwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feel comfortable proceeding with `nn.MultiheadAttention`, knowing exactly how the math is calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
