{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch's nn.MultiheadAttention is designed to be flexible for different use cases, so it requires Q, K, and V as inputs rather than assuming they will be derived from the same source every time. \n",
    "\n",
    "- Notice the input for Q,K,V can be different, which make it flexible to different use cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if `bias` = True and `add_kv_bias` = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q = x \\cdot W^Q + b^Q$\n",
    "\n",
    "$K = x \\cdot W^K + b^K + \\text{bias}_{K_shared}$\n",
    "\n",
    "$V = x \\cdot W^V + b^V + \\text{bias}_{V_shared}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simplicity we'll set both `bias` and `add_kv_bias` to `False` in `nn.MultiheadAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal Hyperparameters for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "num_tokens = 8\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal inputs (with `torch.manual_seed`) for PART ONE and PART TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "input_to_produce_Q = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "input_to_produce_K = torch.rand(batch_size,num_tokens,embed_dim)\n",
    "input_to_produce_V = torch.rand(batch_size,num_tokens,embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART ONE : Pytorch's  Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.2281, -0.3698, -0.1026, -0.2641],\n",
      "        [-0.1962,  0.0293,  0.3651,  0.3328],\n",
      "        [-0.5986,  0.3796,  0.1711,  0.5809],\n",
      "        [ 0.4042, -0.5580, -0.5822, -0.2954],\n",
      "        [ 0.5377, -0.1020,  0.2621, -0.2846],\n",
      "        [ 0.6009, -0.2591,  0.4592,  0.0073],\n",
      "        [-0.3226,  0.3148, -0.3251,  0.1801],\n",
      "        [-0.1768, -0.0671, -0.5887, -0.2920],\n",
      "        [ 0.3323, -0.1489,  0.6100,  0.4909],\n",
      "        [-0.0287, -0.4087,  0.3729,  0.1901],\n",
      "        [-0.3958,  0.3978,  0.3718,  0.5431],\n",
      "        [-0.3433, -0.1008, -0.0119,  0.0894]], requires_grad=True)\n",
      "torch.Size([12, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "THe following is out projection weight\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450]], requires_grad=True)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                       num_heads=num_heads,\n",
    "                                       dropout=0,\n",
    "                                       bias = True,\n",
    "                                       add_bias_kv=False,\n",
    "                                       batch_first=True,    # important\n",
    "                                       device=None)\n",
    "\n",
    "\n",
    "\n",
    "# To grab the weights pytorch's initialized .....\n",
    "print('The following are W_q,W_k,W_v concatenated into one matrix, this is how pytorch initializes the weight matrix\\n')\n",
    "print(multihead_attn.in_proj_weight)\n",
    "print(multihead_attn.in_proj_weight.shape)\n",
    "\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('THe following is out projection weight\\n')\n",
    "print(multihead_attn.out_proj.weight)  # Output projection weights\n",
    "print(multihead_attn.out_proj.weight.shape)  # Output projection weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : pytorch `nn.MultiheadAttention()` does not mask (Q.K^T) by default. we have to pass the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = torch.triu(torch.ones(num_tokens,num_tokens),diagonal=1).bool()\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART ONE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART ONE RESULT :\n",
      "tensor([[[ 0.0546, -0.3211,  0.1117,  0.0680],\n",
      "         [ 0.0543, -0.3043,  0.1045,  0.0583],\n",
      "         [ 0.0645, -0.3686,  0.1079,  0.0824],\n",
      "         [ 0.0657, -0.3751,  0.1066,  0.0920],\n",
      "         [ 0.0610, -0.3521,  0.1016,  0.0831],\n",
      "         [ 0.0655, -0.3559,  0.0966,  0.0876],\n",
      "         [ 0.0621, -0.3573,  0.0956,  0.0895],\n",
      "         [ 0.0596, -0.3492,  0.0888,  0.0911]]])\n",
      "torch.Size([1, 8, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART ONE ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4735, 0.5265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3138, 0.3364, 0.3498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2402, 0.2487, 0.2547, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1835, 0.2049, 0.2169, 0.2079, 0.1869, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1592, 0.1664, 0.1703, 0.1673, 0.1603, 0.1765, 0.0000, 0.0000],\n",
      "         [0.1249, 0.1399, 0.1482, 0.1445, 0.1283, 0.1636, 0.1507, 0.0000],\n",
      "         [0.1023, 0.1168, 0.1248, 0.1264, 0.1075, 0.1425, 0.1287, 0.1510]]])\n",
      "torch.Size([1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    PART_ONE_RESULT, attn_output_weights_part_one  = multihead_attn(input_to_produce_Q,input_to_produce_K,input_to_produce_V,\n",
    "                                                                    attn_mask = attn_mask )\n",
    "\n",
    "print('PART ONE RESULT :')\n",
    "print(PART_ONE_RESULT)\n",
    "print(PART_ONE_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART ONE ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_one)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_one.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the weights which `nn.MultiheadAttention` initialized, in our `FromScratchMultiheadAttention`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART TWO : FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromScratchMultiheadAttention(nn.Module):\n",
    "  def __init__(self,context_window,embed_dim,num_heads,dropout=0,add_bias_kv=False,device=None):\n",
    "    super().__init__()\n",
    "\n",
    "    # we will assume d_in == d_out and they are both embed_dim.\n",
    "\n",
    "    # Handling dimensions\n",
    "    assert embed_dim % num_heads == 0, 'Embedding must be divisible by Number of heads'\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = self.embed_dim//self.num_heads\n",
    "\n",
    "    # W_q, W_k, W_v\n",
    "    self.W_q      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_k      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.W_v      = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "    self.out_proj = nn.Linear(embed_dim,embed_dim,bias=False)\n",
    "\n",
    "\n",
    "    W_q,W_k,W_v = multihead_attn.in_proj_weight.chunk(3)   # pytorch's internal initialization of nn.MultiheadAttention. pytorch initialize all three (q,k,v) in a single matrix.\n",
    "    out_proj = multihead_attn.out_proj.weight\n",
    "\n",
    "    # we gonna put the initialized weight by nn.MultiheadAttention to our layers. so we can see if they will produce the same result\n",
    "    self.W_q.weight.data = W_q\n",
    "    self.W_k.weight.data = W_k\n",
    "    self.W_v.weight.data = W_v\n",
    "    self.out_proj.weight.data = out_proj\n",
    "\n",
    "\n",
    "\n",
    "    # Miscellaneous\n",
    "    self.register_buffer('mask',torch.triu(torch.ones(context_window,context_window),diagonal=1))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "  \n",
    "  def forward(self,input_to_produce_Q,input_to_produce_K,input_to_produce_V):\n",
    "    B_q,num_token_q,embed_dim_q = input_to_produce_Q.shape\n",
    "    B_k,num_token_k,embed_dim_k = input_to_produce_K.shape\n",
    "    B_v,num_token_v,embed_dim_v = input_to_produce_V.shape\n",
    "\n",
    "    Q = self.W_q(input_to_produce_Q) \n",
    "    K = self.W_k(input_to_produce_K) \n",
    "    V = self.W_v(input_to_produce_V) \n",
    "\n",
    "    # splitting \n",
    "    Q = Q.view(B_q,num_token_q,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    K = K.view(B_k,num_token_k,self.num_heads,self.head_dim).transpose(1,2)\n",
    "    V = V.view(B_v,num_token_v,self.num_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "    # QK,mask,softmax,dropout\n",
    "    attn_score = Q @ K.transpose(2,3)\n",
    "    attn_score.masked_fill_(self.mask.bool()[:num_token_q,:num_token_k],-torch.inf)\n",
    "    attn_weight = torch.softmax(attn_score/K.shape[-1]**0.5,dim=-1)\n",
    "    attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "    # context_vec\n",
    "    context_vec = attn_weight @ V\n",
    "\n",
    "    # Putting the heads back together \n",
    "    context_vec = context_vec.transpose(1,2).contiguous().view(B_q,num_token_q,self.embed_dim)    # it doesn't matter which (B) you choose\n",
    "\n",
    "    # projection \n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec,attn_weight\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART TWO RESULT  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can think of context_window as a max num_tokens your model can process at one go. since  we are feeding the model 8 tokens (numz-tokens = 8), context_window anything above 8 will do. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART TWO RESULT :\n",
      "tensor([[[ 0.0546, -0.3211,  0.1117,  0.0680],\n",
      "         [ 0.0543, -0.3043,  0.1045,  0.0583],\n",
      "         [ 0.0645, -0.3686,  0.1079,  0.0824],\n",
      "         [ 0.0657, -0.3751,  0.1066,  0.0920],\n",
      "         [ 0.0610, -0.3521,  0.1016,  0.0831],\n",
      "         [ 0.0655, -0.3559,  0.0966,  0.0876],\n",
      "         [ 0.0621, -0.3573,  0.0956,  0.0895],\n",
      "         [ 0.0596, -0.3492,  0.0888,  0.0911]]])\n",
      "torch.Size([1, 8, 4])\n",
      "--------------------------------------------------------------------------------\n",
      "PART TWO ATTENTION OUTPUT WEIGHTS :\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4982, 0.5018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3322, 0.3297, 0.3381, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2468, 0.2415, 0.2451, 0.2666, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1961, 0.1983, 0.2063, 0.2041, 0.1951, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1642, 0.1645, 0.1671, 0.1681, 0.1646, 0.1715, 0.0000, 0.0000],\n",
      "          [0.1370, 0.1401, 0.1462, 0.1399, 0.1343, 0.1534, 0.1491, 0.0000],\n",
      "          [0.1185, 0.1220, 0.1283, 0.1205, 0.1152, 0.1352, 0.1306, 0.1298]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4488, 0.5512, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2955, 0.3431, 0.3614, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2336, 0.2558, 0.2642, 0.2464, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1708, 0.2114, 0.2275, 0.2117, 0.1786, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1543, 0.1683, 0.1735, 0.1664, 0.1561, 0.1815, 0.0000, 0.0000],\n",
      "          [0.1128, 0.1397, 0.1501, 0.1492, 0.1222, 0.1737, 0.1523, 0.0000],\n",
      "          [0.0862, 0.1115, 0.1212, 0.1324, 0.0999, 0.1497, 0.1269, 0.1721]]]])\n",
      "torch.Size([1, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "mha = FromScratchMultiheadAttention(context_window=1024,   # see the above note.\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    num_heads=num_heads,\n",
    "                                    dropout=0.)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    PART_TWO_RESULT,attn_output_weights_part_two = mha(input_to_produce_Q,input_to_produce_K,input_to_produce_V)\n",
    "\n",
    "print('PART TWO RESULT :')\n",
    "print(PART_TWO_RESULT)\n",
    "print(PART_TWO_RESULT.shape)\n",
    "\n",
    "print('----'*20)\n",
    "\n",
    "print('PART TWO ATTENTION OUTPUT WEIGHTS :')\n",
    "print(attn_output_weights_part_two)    # Notice pytorch doesn't return attn_output_weights for each single head separately, rather it takes the average across the heads. \n",
    "print(attn_output_weights_part_two.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4735, 0.5265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3138, 0.3364, 0.3498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2402, 0.2487, 0.2547, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1835, 0.2049, 0.2169, 0.2079, 0.1869, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1592, 0.1664, 0.1703, 0.1673, 0.1603, 0.1765, 0.0000, 0.0000],\n",
       "         [0.1249, 0.1399, 0.1482, 0.1445, 0.1283, 0.1636, 0.1507, 0.0000],\n",
       "         [0.1023, 0.1168, 0.1248, 0.1264, 0.1075, 0.1425, 0.1287, 0.1510]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights_part_two = attn_output_weights_part_two.mean(dim=1)   # taking average across heads dimension.\n",
    "attn_output_weights_part_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `PART_ONE_RESULT` with `PART_TWO_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[ 0.0546, -0.3211,  0.1117,  0.0680],\n",
      "         [ 0.0543, -0.3043,  0.1045,  0.0583],\n",
      "         [ 0.0645, -0.3686,  0.1079,  0.0824],\n",
      "         [ 0.0657, -0.3751,  0.1066,  0.0920],\n",
      "         [ 0.0610, -0.3521,  0.1016,  0.0831],\n",
      "         [ 0.0655, -0.3559,  0.0966,  0.0876],\n",
      "         [ 0.0621, -0.3573,  0.0956,  0.0895],\n",
      "         [ 0.0596, -0.3492,  0.0888,  0.0911]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[ 0.0546, -0.3211,  0.1117,  0.0680],\n",
      "         [ 0.0543, -0.3043,  0.1045,  0.0583],\n",
      "         [ 0.0645, -0.3686,  0.1079,  0.0824],\n",
      "         [ 0.0657, -0.3751,  0.1066,  0.0920],\n",
      "         [ 0.0610, -0.3521,  0.1016,  0.0831],\n",
      "         [ 0.0655, -0.3559,  0.0966,  0.0876],\n",
      "         [ 0.0621, -0.3573,  0.0956,  0.0895],\n",
      "         [ 0.0596, -0.3492,  0.0888,  0.0911]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{PART_ONE_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{PART_TWO_RESULT}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(PART_ONE_RESULT, PART_TWO_RESULT)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `attn_output_weights_part_one` with `attn_output_weights_part_two`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of PART ONE :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4735, 0.5265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3138, 0.3364, 0.3498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2402, 0.2487, 0.2547, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1835, 0.2049, 0.2169, 0.2079, 0.1869, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1592, 0.1664, 0.1703, 0.1673, 0.1603, 0.1765, 0.0000, 0.0000],\n",
      "         [0.1249, 0.1399, 0.1482, 0.1445, 0.1283, 0.1636, 0.1507, 0.0000],\n",
      "         [0.1023, 0.1168, 0.1248, 0.1264, 0.1075, 0.1425, 0.1287, 0.1510]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "Output of PART TWO :\n",
      "\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4735, 0.5265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3138, 0.3364, 0.3498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2402, 0.2487, 0.2547, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1835, 0.2049, 0.2169, 0.2079, 0.1869, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1592, 0.1664, 0.1703, 0.1673, 0.1603, 0.1765, 0.0000, 0.0000],\n",
      "         [0.1249, 0.1399, 0.1482, 0.1445, 0.1283, 0.1636, 0.1507, 0.0000],\n",
      "         [0.1023, 0.1168, 0.1248, 0.1264, 0.1075, 0.1425, 0.1287, 0.1510]]])\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "PART ONE is equal to PART TWO ? True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output of PART ONE :\\n\\n{attn_output_weights_part_one}')\n",
    "print('---'*30)\n",
    "print(f'Output of PART TWO :\\n\\n{attn_output_weights_part_two}')\n",
    "print('---'*30)\n",
    "print(f'\\nPART ONE is equal to PART TWO ? {torch.allclose(attn_output_weights_part_one, attn_output_weights_part_two)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afterwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feel comfortable proceeding with `nn.MultiheadAttention`, knowing exactly how the math is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
